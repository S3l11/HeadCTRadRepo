{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1zGbOkurWtRHTwNNh00Ty1cl-C32BXH1g","authorship_tag":"ABX9TyOSrUU3YdZQP6Lt3AGPdFYt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import PIL\n","import matplotlib.pyplot as plt\n","import string\n","import os\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import nltk\n","import random\n","import chardet\n","import csv\n","import tensorflow as tf\n","#print(tf.__version__)\n","#import ace_tools as tools\n","import openpyxl\n","#import language_tool_python\n","import torch\n","\n","nltk.download('wordnet')\n","\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding, Add, Concatenate, Reshape, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import load_img, img_to_array, to_categorical, plot_model, pad_sequences\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","from IPython.display import Image\n","from PIL import ImageFile\n","from pickle import dump\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","from rouge import Rouge\n","from nltk.translate.meteor_score import meteor_score, single_meteor_score\n","from openpyxl.styles import Font, PatternFill\n","from transformers import AutoTokenizer, AutoModelForMaskedLM"],"metadata":{"id":"1oaXlYHzDiu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect_encoding(file_path):\n","\n","    with open(file_path, 'rb') as file:\n","        raw_data = file.read()\n","\n","    result = chardet.detect(raw_data)\n","    encoding = result['encoding']\n","\n","    return encoding"],"metadata":{"id":"afNe6ii1VnJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_caption_file(path):\n","    encoding = detect_encoding(path)\n","    captions_dict = {}\n","\n","    with open(path, \"r\", encoding=encoding) as file:\n","\n","        for line in file:\n","            parts = line.strip().split(\"\\t\")\n","            if len(parts) == 2:\n","                image_id, report = parts\n","                captions_dict[image_id] = report.strip()\n","            else:\n","                print(f\"Skipping line due to unexpected format: {line.strip()}\")\n","\n","    return captions_dict"],"metadata":{"id":"JkGSGSv_VrDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_reports_in_groups(captions_dict, group_size=5):\n","    grouped_captions_dict = {}\n","    temp_dict = {}\n","\n","    for key, report in captions_dict.items():\n","        prefix = key[:4]\n","        if prefix not in temp_dict:\n","            temp_dict[prefix] = []\n","\n","        temp_dict[prefix].append((key, report))\n","\n","    for prefix, items in temp_dict.items():\n","\n","        for i in range(0, len(items), group_size):\n","            group = items[i:i + group_size]\n","            if len(group) == group_size:\n","                group_report = group[0][1]\n","\n","                for key, _ in group:\n","                    grouped_captions_dict[key] = group_report\n","\n","    return grouped_captions_dict"],"metadata":{"id":"Sg2M5cS3d54Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_features(directory, image_keys):\n","    model = VGG16()\n","    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","    features = dict()\n","\n","    for name in image_keys:\n","        filename = os.path.join(directory, name + '.jpg')\n","        image = load_img(filename, target_size=(224, 224))\n","        image = img_to_array(image)\n","        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","        image = preprocess_input(image)\n","        feature = model.predict(image, verbose=0)\n","        image_id = name.split('.')[0]\n","        features[image_id] = feature\n","\n","    return features"],"metadata":{"id":"5h04hst9WGHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_data(image_keys, group_size=5):\n","    x1 = [[] for _ in range(group_size)]\n","    x2, y = [], []\n","\n","    for i in range(0, len(image_keys), group_size):\n","        group_keys = image_keys[i:i + group_size]\n","        if len(group_keys) < group_size:\n","            break\n","\n","        group_features = [train_validate_features[image][0] for image in group_keys]\n","        report = train_validate_image_caption[group_keys[0]]\n","        caption_tokens = report.split()\n","        seq = tokenizer.texts_to_sequences([caption_tokens])[0]\n","        length = len(seq)\n","\n","        for k in range(1, length):\n","            x2_seq, y_seq = seq[:k], seq[k]\n","            x2_seq = pad_sequences([x2_seq], maxlen=max_len)[0]\n","            y_seq = to_categorical([y_seq], num_classes=vocab_len)[0]\n","\n","            for idx, feature in enumerate(group_features):\n","                x1[idx].append(feature)\n","\n","            x2.append(x2_seq)\n","            y.append(y_seq)\n","\n","    return [np.array(x) for x in x1], np.array(x2), np.array(y)"],"metadata":{"id":"Kkg-0kVO5PHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def define_model(max_len, vocab_size, group_size=5):\n","    input_images = [Input(shape=(4096,)) for _ in range(group_size)]\n","\n","    concatenated_images = Concatenate()(input_images)\n","    image_features = Dense(4096, activation='relu')(concatenated_images)\n","\n","    input_caption = Input(shape=(max_len,))\n","    reshaped_input_caption = Reshape((max_len, 1))(input_caption)\n","\n","    lstm = LSTM(256)(reshaped_input_caption)\n","\n","    combined_features = Concatenate()([image_features, lstm])\n","\n","    dense = Dense(256, activation='relu')(combined_features)\n","    output = Dense(vocab_size, activation='softmax')(dense)\n","\n","    model = Model(inputs=input_images + [input_caption], outputs=output)\n","\n","    return model"],"metadata":{"id":"xhoi7CfumkpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_features(image_paths):\n","    base_model = VGG16(weights='imagenet')\n","    model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n","\n","    features = {}\n","\n","    for image_path in image_paths:\n","        image = load_img(image_path, target_size=(224, 224))\n","        image = img_to_array(image)\n","        image = preprocess_input(image)\n","\n","        image = np.expand_dims(image, axis=0)\n","\n","        feature = model.predict(image)\n","\n","        image_id = os.path.splitext(os.path.basename(image_path))[0]\n","\n","        features[image_id] = feature\n","\n","    return features"],"metadata":{"id":"4ABhfVL-7jyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_desc(model, tokenizer, photo, max_len, temperature=1.0):\n","    in_text = 'startseq'\n","    predicted_words = []\n","\n","    for _ in range(max_len):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        padded_sequence = pad_sequences([sequence], maxlen=max_len)\n","\n","        inputs = [photo] * 5 + [padded_sequence]\n","\n","        yhat = model.predict(inputs, verbose=0)\n","\n","        yhat = yhat.flatten()\n","\n","        yhat = np.log(yhat + 1e-10) / temperature\n","        yhat = np.exp(yhat) / np.sum(np.exp(yhat))\n","\n","        next_index = np.random.choice(len(yhat), p=yhat)\n","        next_word = tokenizer.index_word.get(next_index, None)\n","        if next_word is not None and next_word != 'endseq':\n","            predicted_words.append(next_word)\n","            in_text += ' ' + next_word\n","        else:\n","            break\n","\n","    prediction = ' '.join(predicted_words).replace(' endseq', '').strip()\n","\n","    return prediction"],"metadata":{"id":"HuUGGXLm7p_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_bleu_scores(reference, hypothesis):\n","    smoothing_function = SmoothingFunction().method1\n","    bleu1 = sentence_bleu([reference], hypothesis, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n","    bleu2 = sentence_bleu([reference], hypothesis, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n","    bleu3 = sentence_bleu([reference], hypothesis, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing_function)\n","    bleu4 = sentence_bleu([reference], hypothesis, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n","\n","    return bleu1, bleu2, bleu3, bleu4"],"metadata":{"id":"ei9FtGIFXnhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_dataset_path = '' # dir containing slices (i.e., XXXX_SliceYYYY)\n","caption_dataset_path = '' # file containing reports"],"metadata":{"id":"_IAOHi6eDWj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["captions_dict = load_caption_file(caption_dataset_path)\n","\n","grouped_captions_dict = process_reports_in_groups(captions_dict)"],"metadata":{"id":"KY45g--3K1ah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_captions_dict = {}\n","\n","table = str.maketrans('', '', string.punctuation)\n","\n","for caption_id, caption_text in grouped_captions_dict.items():\n","    cleaned_caption = caption_text.split()\n","    cleaned_caption = [token.lower() for token in cleaned_caption]\n","    cleaned_caption = [token.translate(table) for token in cleaned_caption]\n","    cleaned_caption = [token for token in cleaned_caption if len(token) > 1]\n","    cleaned_caption = ' '.join(cleaned_caption)\n","    cleaned_caption = 'startseq ' + cleaned_caption + ' endseq'\n","    new_captions_dict[caption_id] = cleaned_caption"],"metadata":{"id":"nRoO1xWyk2_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["caption_images_list = [image.split('.')[0] for image in os.listdir(image_dataset_path) if image.split('.')[0] in new_captions_dict]\n","prefix_groups = {}\n","\n","for image in caption_images_list:\n","    prefix = image[:4]\n","    if prefix not in prefix_groups:\n","        prefix_groups[prefix] = []\n","\n","    prefix_groups[prefix].append(image)\n","\n","grouped_images = [group for group in prefix_groups.values() if len(group) == 5]"],"metadata":{"id":"jvOXEhiA9yWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.seed(12)\n","random.shuffle(grouped_images)\n","\n","flattened_images = [image for group in grouped_images for image in group]\n","\n","num_test_groups = int(0.20 * len(grouped_images))\n","\n","test_groups = grouped_images[:num_test_groups]\n","train_validate_groups = grouped_images[num_test_groups:]\n","\n","test_images = [image for group in test_groups for image in group]\n","train_validate_images = [image for group in train_validate_groups for image in group]\n","\n","random.shuffle(train_validate_images)\n","random.shuffle(test_images)"],"metadata":{"id":"CtU67np8GJL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ImageFile.LOAD_TRUNCATED_IMAGES = True\n","train_validate_features = extract_features(image_dataset_path, train_validate_images)\n","\n","with open(r'.../train-val-features.pkl', 'wb') as f:\n","    dump(train_validate_features, f)"],"metadata":{"id":"msfX2l7Xm7KA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(r'.../train-val-features.pkl', 'rb') as file:\n","    train_validate_features = pickle.load(file)"],"metadata":{"id":"Uta25MYIaCaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_validate_image_caption = {image: new_captions_dict[image] for image in train_validate_images if image in train_validate_features}\n","\n","tokenizer = Tokenizer()\n","all_captions = list(new_captions_dict.values())\n","tokenizer.fit_on_texts(all_captions)\n","\n","vocab_len = len(tokenizer.word_index) + 1\n","max_len = max(len(caption.split()) for caption in all_captions)"],"metadata":{"id":"VIE4WXyCHYoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_train_validate = len(train_validate_images)\n","\n","num_validate_images = int(0.15 * total_train_validate)\n","num_train_images = total_train_validate - num_validate_images\n","\n","train_x1, train_x2, train_y = prepare_data(train_validate_images[:num_train_images], group_size=5)\n","validate_x1, validate_x2, validate_y = prepare_data(train_validate_images[num_train_images:], group_size=5)"],"metadata":{"id":"UdnZn43hH9IM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = vocab_len\n","model = define_model(max_len, vocab_size, group_size=5)\n","\n","learning_rate = 0.001\n","optimizer = Adam(learning_rate=learning_rate)\n","\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy')"],"metadata":{"id":"VaE2JO67Wwgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["callbacks = [EarlyStopping(patience=8)]\n","\n","history = model.fit(train_x1 + [train_x2],\n","                    train_y,\n","                    verbose=1,\n","                    epochs=100,\n","                    batch_size=32,\n","                    shuffle=True,\n","                    callbacks=callbacks,\n","                    validation_data=(validate_x1 + [validate_x2], validate_y))\n","\n","model.save('.../model-weights.h5')"],"metadata":{"id":"C0IXb-KatUxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 10))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('LEARNING CURVES')\n","plt.xlabel('EPOCHS')\n","plt.ylabel('LOSS')\n","plt.legend(['Loss Train', 'Loss Val'], loc='upper right')\n","plt.show()"],"metadata":{"id":"9tZp5aIDSZUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csv_file_path = '.../output-semantic.csv'\n","\n","with open(csv_file_path, 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['Image Prefix', 'Predicted Report', 'Actual Report'])\n","\n","    prefix_groups = {}\n","\n","    for image in test_images:\n","        prefix = image[:4]\n","        if prefix not in prefix_groups:\n","            prefix_groups[prefix] = []\n","\n","        prefix_groups[prefix].append(image)\n","\n","    for prefix, images in prefix_groups.items():\n","        print(f\"Slices of scan number: {prefix}\")\n","\n","        image_paths = [os.path.join(image_dataset_path, image + '.jpg') for image in images]\n","        image_features = extract_features(image_paths)\n","\n","        photos = [np.array([image_features[image][0]]) for image in images]\n","\n","        actual_report = new_captions_dict[images[0]]\n","\n","        generated_report = generate_desc(model, tokenizer, photos[0], max_len)\n","\n","        print(f\"Predicted Report: {generated_report}\")\n","        print(f\"Actual Report: {actual_report}\")\n","        print('------------')\n","\n","        writer.writerow([prefix, generated_report, actual_report])"],"metadata":{"id":"6tYUwbG77Yy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csv_file_path = '.../output-semantic.csv'\n","df = pd.read_csv(csv_file_path)\n","\n","file_path = '.../output-semantic.xlsx'\n","df.to_excel(file_path, index=False, engine='openpyxl')\n","df = pd.read_excel(file_path)\n","\n","rouge = Rouge()\n","\n","bleu1_scores = []\n","bleu2_scores = []\n","bleu3_scores = []\n","bleu4_scores = []\n","rouge1_scores = []\n","rouge2_scores = []\n","rougeL_scores = []\n","meteor_scores = []\n","\n","for index, row in df.iterrows():\n","    if pd.notnull(row['Actual Report']) and pd.notnull(row['Predicted Report']):\n","        reference = row['Actual Report'].split()\n","        hypothesis = row['Predicted Report'].split()\n","\n","        bleu1, bleu2, bleu3, bleu4 = calculate_bleu_scores(reference, hypothesis)\n","        bleu1_scores.append(bleu1)\n","        bleu2_scores.append(bleu2)\n","        bleu3_scores.append(bleu3)\n","        bleu4_scores.append(bleu4)\n","\n","        rouge_scores = rouge.get_scores(' '.join(hypothesis), ' '.join(reference))[0]\n","        rouge1_scores.append(rouge_scores['rouge-1']['f'])\n","        rouge2_scores.append(rouge_scores['rouge-2']['f'])\n","        rougeL_scores.append(rouge_scores['rouge-l']['f'])\n","\n","        meteor = meteor_score([reference], hypothesis)\n","        meteor_scores.append(meteor)\n","    else:\n","        bleu1_scores.append(0)\n","        bleu2_scores.append(0)\n","        bleu3_scores.append(0)\n","        bleu4_scores.append(0)\n","        rouge1_scores.append(0)\n","        rouge2_scores.append(0)\n","        rougeL_scores.append(0)\n","        meteor_scores.append(0)\n","\n","df['BLEU-1'] = bleu1_scores\n","df['BLEU-2'] = bleu2_scores\n","df['BLEU-3'] = bleu3_scores\n","df['BLEU-4'] = bleu4_scores\n","df['ROUGE-1'] = rouge1_scores\n","df['ROUGE-2'] = rouge2_scores\n","df['ROUGE-L'] = rougeL_scores\n","df['METEOR'] = meteor_scores\n","\n","global_bleu1 = df['BLEU-1'].mean()\n","global_bleu2 = df['BLEU-2'].mean()\n","global_bleu3 = df['BLEU-3'].mean()\n","global_bleu4 = df['BLEU-4'].mean()\n","global_rouge1 = df['ROUGE-1'].mean()\n","global_rouge2 = df['ROUGE-2'].mean()\n","global_rougeL = df['ROUGE-L'].mean()\n","global_meteor = df['METEOR'].mean()\n","\n","df.loc['Global Average'] = [''] * (len(df.columns) - 8) + [global_bleu1, global_bleu2, global_bleu3, global_bleu4, global_rouge1, global_rouge2, global_rougeL, global_meteor]\n","\n","output_file_path = '.../output-semantic&scores.xlsx'\n","df.to_excel(output_file_path, index=False)\n","\n","wb = openpyxl.load_workbook(output_file_path)\n","ws = wb.active\n","\n","last_row = ws.max_row\n","red_font = Font(color=\"FF0000\", bold=True)\n","for cell in ws[last_row]:\n","    cell.font = red_font\n","\n","wb.save(output_file_path)\n","\n","print(df.head())"],"metadata":{"id":"8h6JpHYZF-dY"},"execution_count":null,"outputs":[]}]}