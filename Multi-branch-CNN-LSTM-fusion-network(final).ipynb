{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1_pi6dyn7-VF9mc19TCDmgMq2354eqauQ","authorship_tag":"ABX9TyM5pev+MBsRxOHJL8ETZIJh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import PIL\n","import matplotlib.pyplot as plt\n","import string\n","import os\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import nltk\n","import random\n","import chardet\n","import csv\n","import tensorflow as tf\n","#print(tf.__version__) # 2.15\n","#import ace_tools as tools\n","import openpyxl\n","#import language_tool_python\n","import torch\n","\n","nltk.download('wordnet')\n","\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding, Add, Concatenate, Reshape, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import load_img, img_to_array, to_categorical, plot_model, pad_sequences\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","from IPython.display import Image\n","from PIL import ImageFile\n","from pickle import dump\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","from rouge import Rouge\n","from nltk.translate.meteor_score import meteor_score, single_meteor_score\n","from openpyxl.styles import Font, PatternFill\n","from transformers import AutoTokenizer, AutoModelForMaskedLM"],"metadata":{"id":"1oaXlYHzDiu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect_encoding(file_path):\n","\n","    with open(file_path, 'rb') as file:\n","        raw_data = file.read()\n","\n","    result = chardet.detect(raw_data)\n","    encoding = result['encoding']\n","\n","    return encoding"],"metadata":{"id":"RiHQwVqy8Yvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_caption_file(path):\n","    encoding = detect_encoding(path)\n","    captions_dict = {}\n","\n","    with open(path, \"r\", encoding=encoding) as file:\n","\n","        for line in file:\n","            parts = line.strip().split(\"\\t\")\n","            if len(parts) == 2:\n","                image_id, report = parts\n","                captions = [caption.strip() for caption in report.split(\".\") if caption.strip()]\n","                captions_dict[image_id] = captions\n","            else:\n","                print(f\"Skipping line due to unexpected format: {line.strip()}\")\n","\n","    return captions_dict"],"metadata":{"id":"DMLrGQNi8gBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_reports_in_groups(captions_dict, group_size=5):\n","    grouped_captions_dict = {}\n","    temp_dict = {}\n","\n","    for key, report in captions_dict.items():\n","        prefix = key[:4]\n","        if prefix not in temp_dict:\n","            temp_dict[prefix] = []\n","\n","        temp_dict[prefix].append((key, report))\n","\n","    for prefix, items in temp_dict.items():\n","\n","        for i in range(0, len(items), group_size):\n","            group = items[i:i + group_size]\n","            if len(group) == group_size:\n","                group_report = group[0][1]\n","\n","                for key, _ in group:\n","                    grouped_captions_dict[key] = group_report\n","\n","    return grouped_captions_dict"],"metadata":{"id":"1ecHpmoG8okv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_features(directory, image_keys):\n","    model = VGG16()\n","    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","    features = dict()\n","\n","    for name in image_keys:\n","        filename = os.path.join(directory, name + '.jpg')\n","        image = load_img(filename, target_size=(224, 224))\n","        image = img_to_array(image)\n","        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","        image = preprocess_input(image)\n","        feature = model.predict(image, verbose=0)\n","        image_id = name.split('.')[0]\n","        features[image_id] = feature\n","\n","    return features"],"metadata":{"id":"owiyFAYV9yGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_data(image_keys, Nmax, group_size=5):\n","    x1 = [[] for _ in range(group_size)]\n","    x2, y = [[] for _ in range(Nmax)], [[] for _ in range(Nmax)]\n","\n","    for i in range(0, len(image_keys), group_size):\n","        group_keys = image_keys[i:i + group_size]\n","        if len(group_keys) < group_size:\n","            break\n","\n","        group_features = [train_validate_features[image][0] for image in group_keys]\n","\n","        for image in group_keys:\n","            captions_list = train_validate_image_caption[image]\n","            num_captions = len(captions_list)\n","\n","            for j, caption in enumerate(captions_list):\n","                caption_tokens = caption.split()\n","                seq = tokenizer.texts_to_sequences([caption_tokens])[0]\n","                length = len(seq)\n","\n","                for k in range(1, length):\n","                    x2_seq, y_seq = seq[:k], seq[k]\n","                    x2_seq = pad_sequences([x2_seq], maxlen=max_len)[0]\n","                    y_seq = to_categorical([y_seq], num_classes=vocab_len)[0]\n","\n","                    for idx, feature in enumerate(group_features):\n","                        x1[idx].append(feature)\n","\n","                    for m in range(Nmax):\n","                        if m == j:\n","                            x2[m].append(x2_seq)\n","                            y[m].append(y_seq)\n","                        else:\n","                            x2[m].append(pad_sequences([[]], maxlen=max_len)[0])\n","                            y[m].append(to_categorical([0], num_classes=vocab_len)[0])\n","\n","            if num_captions < Nmax:\n","                padding_needed = Nmax - num_captions\n","\n","                for j in range(num_captions, Nmax):\n","                    x2_seq = pad_sequences([[]], maxlen=max_len)[0]\n","                    y_seq = to_categorical([0], num_classes=vocab_len)[0]\n","\n","                    for idx, feature in enumerate(group_features):\n","                        x1[idx].append(feature)\n","\n","                    for m in range(Nmax):\n","                        x2[m].append(x2_seq)\n","                        y[m].append(y_seq)\n","\n","    return [np.array(x) for x in x1], [np.array(x) for x in x2], [np.array(y_seq) for y_seq in y]"],"metadata":{"id":"g2dRwu7VvPx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def define_model(max_len, vocab_size, Nmax, group_size=5):\n","    input_images = [Input(shape=(4096,)) for _ in range(group_size)]\n","\n","    concatenated_images = Concatenate()(input_images)\n","    image_features = Dense(4096, activation='relu')(concatenated_images)\n","\n","    input_captions = []\n","    lstm_outputs = []\n","    outputs = []\n","\n","    for i in range(Nmax):\n","        input_caption = Input(shape=(max_len,))\n","        input_captions.append(input_caption)\n","\n","        reshaped_input_caption = Reshape((max_len, 1))(input_caption)\n","\n","        lstm = LSTM(256, name=f'lstm_{i}')(reshaped_input_caption)\n","        lstm_outputs.append(lstm)\n","\n","        combined_features = Concatenate()([image_features, lstm])\n","\n","        dense = Dense(256, activation='relu')(combined_features)\n","        output = Dense(vocab_size, activation='softmax')(dense)\n","        outputs.append(output)\n","\n","    model = Model(inputs=input_images + input_captions, outputs=outputs)\n","\n","    return model"],"metadata":{"id":"XfhOjcf7_Z91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_learning_curves(history, Nmax):\n","    plt.figure(figsize=(12, 10))\n","    label_count = 1\n","\n","    for i in range(2, Nmax * 2 + 1, 2):\n","        plt.plot(history.history[f'dense_{i}_loss'], label=f'Loss Train LSTM {label_count}')\n","        plt.plot(history.history[f'val_dense_{i}_loss'], label=f'Loss Val LSTM {label_count}', linestyle='--')\n","\n","        label_count += 1\n","\n","    plt.title('LEARNING CURVES')\n","    plt.xlabel('EPOCHS')\n","    plt.ylabel('LOSS')\n","    plt.legend(loc='upper right')\n","    plt.show()"],"metadata":{"id":"2V7H_f6_DN7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_features(image_paths):\n","    base_model = VGG16()\n","    model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n","\n","    features = {}\n","\n","    for image_path in image_paths:\n","        image = load_img(image_path, target_size=(224, 224))\n","        image = img_to_array(image)\n","        image = preprocess_input(image)\n","\n","        image = np.expand_dims(image, axis=0)\n","\n","        feature = model.predict(image)\n","\n","        image_id = os.path.splitext(os.path.basename(image_path))[0]\n","\n","        features[image_id] = feature\n","\n","    return features"],"metadata":{"id":"4BqPx1GuJYQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def words_for_id(integer, tokenizers):\n","    matching_words = []\n","\n","    for tokenizer in tokenizers:\n","        if integer in tokenizer.word_index:\n","            matching_words.append(tokenizer.index_word[integer])\n","\n","    return matching_words"],"metadata":{"id":"_BwYwMl6vcFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_desc(model, tokenizers, photo, max_len, temperature=1.0):\n","    all_predictions = []\n","\n","    for j, tokenizer in enumerate(tokenizers):\n","        in_text = 'startseq'\n","        predicted_words = []\n","\n","        for _ in range(max_len):\n","            sequence = tokenizer.texts_to_sequences([in_text])[0]\n","            padded_sequence = pad_sequences([sequence], maxlen=82)\n","\n","            inputs = [photo] * 5\n","            inputs += [np.zeros((1, 82)) for _ in range(9)]\n","\n","            inputs[5 + j] = padded_sequence\n","\n","            yhat = model.predict(inputs, verbose=0)\n","\n","            yhat = yhat[j]\n","\n","            yhat = yhat.flatten()\n","\n","            yhat = np.log(yhat + 1e-10) / temperature\n","            yhat = np.exp(yhat) / np.sum(np.exp(yhat))\n","\n","            next_index = np.random.choice(len(yhat), p=yhat)\n","            next_word = tokenizer.index_word.get(next_index, None)\n","            if next_word is not None and next_word != 'endseq':\n","                predicted_words.append(next_word)\n","                in_text += ' ' + next_word\n","            else:\n","                break\n","\n","        prediction = ' '.join(predicted_words).replace(' endseq', '').strip()\n","        all_predictions.append(prediction)\n","\n","    return all_predictions"],"metadata":{"id":"Qb_nxxh-NzOd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_semantic_score(prediction):\n","    inputs = bert_tokenizer(prediction, return_tensors=\"pt\", truncation=True, max_length=512)\n","\n","    with torch.no_grad():\n","        outputs = bert_model(**inputs, labels=inputs[\"input_ids\"])\n","        loss = outputs.loss\n","        perplexity = torch.exp(loss).item()\n","\n","    return -perplexity"],"metadata":{"id":"mHlWAGl1BNdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_heuristic_score(prediction):\n","\n","    if prediction.strip() == \"\":\n","        return -float('inf')\n","\n","    return calculate_semantic_score(prediction)"],"metadata":{"id":"O4juT6OMBRfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_bleu_scores(reference, hypothesis):\n","    smoothing_function = SmoothingFunction().method1\n","    bleu1 = sentence_bleu([reference], hypothesis, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n","    bleu2 = sentence_bleu([reference], hypothesis, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n","    bleu3 = sentence_bleu([reference], hypothesis, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing_function)\n","    bleu4 = sentence_bleu([reference], hypothesis, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n","\n","    return bleu1, bleu2, bleu3, bleu4"],"metadata":{"id":"M25NUZzJCMx1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_dataset_path = '' # dir containing slices (i.e., XXXX_SliceYYYY)\n","caption_dataset_path = '' # file containing reports"],"metadata":{"id":"_IAOHi6eDWj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["captions_dict = load_caption_file(caption_dataset_path)\n","\n","grouped_captions_dict = process_reports_in_groups(captions_dict)"],"metadata":{"id":"KY45g--3K1ah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_captions_dict = {}\n","\n","table = str.maketrans('', '', string.punctuation)\n","\n","for caption_id, caption_list in grouped_captions_dict.items():\n","    cleaned_captions = []\n","\n","    for caption_text in caption_list:\n","        caption_text = caption_text.split()\n","        caption_text = [token.lower() for token in caption_text]\n","        caption_text = [token.translate(table) for token in caption_text]\n","        caption_text = [token for token in caption_text if len(token) > 1]\n","        cleaned_caption = ' '.join(caption_text)\n","        cleaned_caption = 'startseq ' + cleaned_caption + ' endseq'\n","        cleaned_captions.append(cleaned_caption)\n","\n","    new_captions_dict[caption_id] = cleaned_captions\n","\n","del grouped_captions_dict"],"metadata":{"id":"nRoO1xWyk2_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Nmax = max(len(captions) for captions in new_captions_dict.values())"],"metadata":{"id":"60HtWwFM-sGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["caption_images_list = []\n","image_index = list(new_captions_dict.keys())\n","caption_images_list = [image.split('.')[0] for image in os.listdir(image_dataset_path) if image.split('.')[0] in image_index]"],"metadata":{"id":"uzVN5binEkcC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prefix_groups = {}\n","\n","for image in caption_images_list:\n","    prefix = image[:4]\n","    if prefix not in prefix_groups:\n","        prefix_groups[prefix] = []\n","\n","    prefix_groups[prefix].append(image)\n","\n","grouped_images = []\n","\n","for group in prefix_groups.values():\n","    if len(group) == 5:\n","        grouped_images.append(group)"],"metadata":{"id":"jvOXEhiA9yWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.seed(12)\n","random.shuffle(grouped_images)\n","\n","flattened_images = [image for group in grouped_images for image in group]\n","\n","num_test_groups = int(0.20 * len(grouped_images))\n","\n","test_groups = grouped_images[:num_test_groups]\n","train_validate_groups = grouped_images[num_test_groups:]\n","\n","test_images = [image for group in test_groups for image in group]\n","train_validate_images = [image for group in train_validate_groups for image in group]\n","\n","random.shuffle(train_validate_images)\n","random.shuffle(test_images)"],"metadata":{"id":"CtU67np8GJL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ImageFile.LOAD_TRUNCATED_IMAGES = True\n","train_validate_features = extract_features(image_dataset_path, train_validate_images)\n","\n","with open(r'.../train-val-features.pkl', 'wb') as f:\n","    dump(train_validate_features, f)"],"metadata":{"id":"ttfFEWnV7I42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(r'.../train-val-features.pkl', 'rb') as file:\n","    train_validate_features = pickle.load(file)"],"metadata":{"id":"0HCD8YMIaOYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_validate_image_caption = {}\n","\n","for image, caption in new_captions_dict.items():\n","    if image in train_validate_images and image in list(train_validate_features.keys()):\n","        train_validate_image_caption.update({image: caption})\n","\n","len(train_validate_image_caption)"],"metadata":{"id":"VIE4WXyCHYoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","\n","all_captions = [caption for captions_list in new_captions_dict.values() for caption in captions_list]\n","tokenizer.fit_on_texts(all_captions)\n","\n","vocab_len = len(tokenizer.word_index) + 1\n","\n","max_len = max(\n","    max(len(caption.split()) for caption in captions_list) for captions_list in train_validate_image_caption.values())"],"metadata":{"id":"1oQniZtOHpXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_train_validate = len(train_validate_images)\n","\n","num_validate_images = int(0.15 * total_train_validate)\n","num_train_images = total_train_validate - num_validate_images\n","\n","train_x1, train_x2, train_y = prepare_data(train_validate_images[:num_train_images], Nmax, group_size=5)\n","validate_x1, validate_x2, validate_y = prepare_data(train_validate_images[num_train_images:], Nmax, group_size=5)"],"metadata":{"id":"UdnZn43hH9IM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = vocab_len\n","model = define_model(max_len, vocab_size, Nmax, group_size=5)\n","\n","learning_rate = 0.001\n","optimizer = Adam(learning_rate=learning_rate)\n","\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n","\n","print(model.summary())"],"metadata":{"id":"xhoi7CfumkpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["callbacks = [EarlyStopping(patience=8)]\n","\n","history = model.fit(train_x1 + train_x2,\n","                    train_y,\n","                    verbose=1,\n","                    epochs=100,\n","                    batch_size=32,\n","                    shuffle=True,\n","                    callbacks=callbacks,\n","                    validation_data=(validate_x1 + validate_x2, validate_y))\n","\n","model.save('.../model-weights.h5')"],"metadata":{"id":"C0IXb-KatUxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_learning_curves(history, Nmax)"],"metadata":{"id":"vymfAY-HHh6_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 10))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('LEARNING CURVES')\n","plt.xlabel('EPOCHS')\n","plt.ylabel('LOSS')\n","plt.legend(['Loss Train', 'Loss Val'], loc='upper right')\n","plt.show()"],"metadata":{"id":"9tZp5aIDSZUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = '.../model-weights.h5'\n","model = load_model(model_path)\n","\n","for i, layer in enumerate(model.inputs):\n","    print(f\"Input {i+1}: {layer.shape}\")\n","\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","bert_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n","\n","tokenizers = [Tokenizer() for _ in range(9)]\n","for tokenizer in tokenizers:\n","    tokenizer.fit_on_texts([caption for _, captions in new_captions_dict.items() for caption in captions])\n","\n","predicted_captions = []\n","actual_captions = []\n","image_names = []\n","\n","csv_file_path = '.../output-semantic.csv'\n","\n","with open(csv_file_path, 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['Image Prefix', 'Predicted Report', 'Actual Report'])\n","\n","    prefix_groups = {}\n","\n","    for image in test_images:\n","        prefix = image[:4]\n","        if prefix not in prefix_groups:\n","            prefix_groups[prefix] = []\n","\n","        prefix_groups[prefix].append(image)\n","\n","    for prefix, images in prefix_groups.items():\n","        print(f\"Slices of scan number: {prefix}\")\n","        all_captions_list = [new_captions_dict[image] for image in images]\n","\n","        image_paths = [os.path.join(image_dataset_path, image + '.jpg') for image in images]\n","        image_features = extract_features(image_paths)\n","\n","        photos = [np.array([image_features[image][0]]) for image in images]\n","\n","        group_predictions = [[] for _ in range(9)]\n","\n","        for photo, image in zip(photos, images):\n","            predictions = generate_desc(model, tokenizers, photo, max_len)\n","\n","            captions_list = new_captions_dict[image]\n","            num_actual_captions = len(captions_list)\n","            if num_actual_captions < 9:\n","\n","                for i in range(num_actual_captions, 9):\n","                    predictions[i] = \"\"\n","\n","            for i in range(9):\n","                if i < len(predictions):\n","                    group_predictions[i].append(predictions[i])\n","\n","            for j, prediction in enumerate(predictions):\n","                if j < num_actual_captions:\n","                    print(f\"Prediction for predictor {j + 1}: {prediction}\")\n","                else:\n","                    print(f\"Prediction for predictor {j + 1}: (empty)\")\n","            print('---')\n","\n","        best_predictions = [\"\"] * 9\n","        best_scores = [-1] * 9\n","\n","        for i in range(9):\n","            best_prediction_for_position = None\n","            found_valid_prediction = False\n","\n","            for prediction in group_predictions[i]:\n","                if not prediction.strip():\n","                    continue\n","\n","                found_valid_prediction = True\n","\n","                heuristic_score = calculate_heuristic_score(prediction)\n","\n","                if heuristic_score > best_scores[i]:\n","                    best_scores[i] = heuristic_score\n","                    best_prediction_for_position = prediction\n","\n","            if not found_valid_prediction or best_prediction_for_position is None:\n","                if group_predictions[i]:\n","                    best_prediction_for_position = group_predictions[i][0]\n","                else:\n","                    best_prediction_for_position = \"\"\n","\n","            best_predictions[i] = best_prediction_for_position\n","\n","        predicted_captions.append(best_predictions)\n","\n","        actual_captions.append([word for word in all_captions_list[0][0].split() if word not in ['startseq', 'endseq']])\n","\n","        print()\n","        print(f\"Best prediction for slices of scan number {prefix}: {best_predictions}\")\n","        print()\n","        print(\"Predicted -> \", best_predictions)\n","        print(\"Actual -> \", [' '.join([word for word in caption.split() if word not in ['startseq', 'endseq']]) for caption in all_captions_list[0]])\n","        print('*********************************************************************')\n","        print()\n","\n","        image_names.append(prefix)\n","        filtered_preds = [element for element in best_predictions if element]\n","        best_preds = \" \".join(filtered_preds)\n","        actuals = \" \".join([' '.join([word for word in caption.split() if word not in ['startseq', 'endseq']]) for caption in all_captions_list[0]])\n","\n","        writer.writerow([prefix, best_preds, actuals])"],"metadata":{"id":"B4nm5XFvVNmZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csv_file_path = '.../output-semantic.csv'\n","df = pd.read_csv(csv_file_path)\n","\n","file_path = '.../output-semantic.xlsx'\n","df.to_excel(file_path, index=False, engine='openpyxl')\n","df = pd.read_excel(file_path)\n","\n","rouge = Rouge()\n","\n","bleu1_scores = []\n","bleu2_scores = []\n","bleu3_scores = []\n","bleu4_scores = []\n","rouge1_scores = []\n","rouge2_scores = []\n","rougeL_scores = []\n","meteor_scores = []\n","\n","for index, row in df.iterrows():\n","    reference = row['Actual Report'].split()\n","    hypothesis = row['Predicted Report'].split()\n","\n","    bleu1, bleu2, bleu3, bleu4 = calculate_bleu_scores(reference, hypothesis)\n","    bleu1_scores.append(bleu1)\n","    bleu2_scores.append(bleu2)\n","    bleu3_scores.append(bleu3)\n","    bleu4_scores.append(bleu4)\n","\n","    rouge_scores = rouge.get_scores(' '.join(hypothesis), ' '.join(reference))[0]\n","    rouge1_scores.append(rouge_scores['rouge-1']['f'])\n","    rouge2_scores.append(rouge_scores['rouge-2']['f'])\n","    rougeL_scores.append(rouge_scores['rouge-l']['f'])\n","\n","    meteor = meteor_score([reference], hypothesis)\n","    meteor_scores.append(meteor)\n","\n","df['BLEU-1'] = bleu1_scores\n","df['BLEU-2'] = bleu2_scores\n","df['BLEU-3'] = bleu3_scores\n","df['BLEU-4'] = bleu4_scores\n","df['ROUGE-1'] = rouge1_scores\n","df['ROUGE-2'] = rouge2_scores\n","df['ROUGE-L'] = rougeL_scores\n","df['METEOR'] = meteor_scores\n","\n","global_bleu1 = df['BLEU-1'].mean()\n","global_bleu2 = df['BLEU-2'].mean()\n","global_bleu3 = df['BLEU-3'].mean()\n","global_bleu4 = df['BLEU-4'].mean()\n","global_rouge1 = df['ROUGE-1'].mean()\n","global_rouge2 = df['ROUGE-2'].mean()\n","global_rougeL = df['ROUGE-L'].mean()\n","global_meteor = df['METEOR'].mean()\n","\n","df.loc['Global Average'] = [''] * (len(df.columns) - 8) + [global_bleu1, global_bleu2, global_bleu3, global_bleu4, global_rouge1, global_rouge2, global_rougeL, global_meteor]\n","\n","output_file_path = '.../output-semantic&scores.xlsx'\n","df.to_excel(output_file_path, index=False)\n","\n","wb = openpyxl.load_workbook(output_file_path)\n","ws = wb.active\n","\n","last_row = ws.max_row\n","red_font = Font(color=\"FF0000\", bold=True)\n","\n","for cell in ws[last_row]:\n","    cell.font = red_font\n","\n","wb.save(output_file_path)\n","\n","print(df.head())"],"metadata":{"id":"yydCJKVHqkLQ"},"execution_count":null,"outputs":[]}]}